{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "998703c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "93330d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f85dd04f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PURCHASE INTENT PREDICTION - DUAL OUTPUT PIPELINE\n",
      "============================================================\n",
      "This pipeline creates two submission files:\n",
      "1. submission_probability.csv - Probability predictions (0-1)\n",
      "2. submission_binary.csv - Binary predictions (0 or 1)\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"PURCHASE INTENT PREDICTION - DUAL OUTPUT PIPELINE\")\n",
    "print(\"=\" * 60)\n",
    "print(\"This pipeline creates two submission files:\")\n",
    "print(\"1. submission_probability.csv - Probability predictions (0-1)\")\n",
    "print(\"2. submission_binary.csv - Binary predictions (0 or 1)\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a8d86939",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 1: Loading datasets...\n",
      "Training data shape: (177086, 23)\n",
      "Test data shape: (44272, 23)\n",
      "Sample submission shape: (4, 2)\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "print(\"\\nStep 1: Loading datasets...\")\n",
    "train_df = pd.read_excel('kaggle/Training_Dataset.xlsx')\n",
    "test_df = pd.read_excel('kaggle/Testing_Dateset.xlsx')\n",
    "sample_submission = pd.read_excel('kaggle/sample_Submission.xlsx')\n",
    "\n",
    "print(f\"Training data shape: {train_df.shape}\")\n",
    "print(f\"Test data shape: {test_df.shape}\")\n",
    "print(f\"Sample submission shape: {sample_submission.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b4f106b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test IDs corrected: 177087 to 221358\n"
     ]
    }
   ],
   "source": [
    "# Correct test IDs to continue from training data\n",
    "test_df['ID'] = range(177087, 177087 + len(test_df))\n",
    "print(f\"Test IDs corrected: {test_df['ID'].min()} to {test_df['ID'].max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "42071da2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Target distribution: Class 0=101421, Class 1=75665\n",
      "Class balance: 42.7% positive class\n"
     ]
    }
   ],
   "source": [
    "# Extract and validate target variable\n",
    "y = train_df['Result'].values.astype(int)\n",
    "print(f\"\\nTarget distribution: Class 0={sum(y==0)}, Class 1={sum(y==1)}\")\n",
    "print(f\"Class balance: {sum(y==1)/len(y)*100:.1f}% positive class\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "935e7d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature engineering function\n",
    "def engineer_features(df):\n",
    "    df = df.copy()\n",
    "\n",
    "    # Date features\n",
    "    for col in ['Start Date', 'End Date']:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_datetime(df[col], errors='coerce')\n",
    "\n",
    "    if 'Start Date' in df.columns and 'End Date' in df.columns:\n",
    "        _extracted_from_engineer_features_10(df)\n",
    "\n",
    "\n",
    "# TODO Rename this here and in `engineer_features`\n",
    "def _extracted_from_engineer_features_10(df):\n",
    "    df['Journey_Duration'] = (df['End Date'] - df['Start Date']).dt.days\n",
    "    df['Journey_Duration'] = df['Journey_Duration'].fillna(0).clip(lower=0, upper=365)\n",
    "\n",
    "    df['Start_Year'] = df['Start Date'].dt.year.fillna(2021)\n",
    "    df['Start_Month'] = df['Start Date'].dt.month.fillna(6)\n",
    "    df['Start_Quarter'] = df['Start Date'].dt.quarter.fillna(2)\n",
    "    df['Start_DayOfWeek'] = df['Start Date'].dt.dayofweek.fillna(3)\n",
    "\n",
    "    df['End_Year'] = df['End Date'].dt.year.fillna(2021)\n",
    "    df['End_Month'] = df['End Date'].dt.month.fillna(6)\n",
    "    df['End_Quarter'] = df['End Date'].dt.quarter.fillna(2)\n",
    "    df['End_DayOfWeek'] = df['End Date'].dt.dayofweek.fillna(3)\n",
    "\n",
    "    df = df.drop(['Start Date', 'End Date'], axis=1)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7226ccf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 1: Loading datasets...\n",
      "Training data shape: (177086, 23)\n",
      "Test data shape: (44272, 23)\n",
      "Sample submission shape: (4, 2)\n",
      "Test IDs corrected: 177087 to 221358\n",
      "\n",
      "Target distribution: Class 0=101421, Class 1=75665\n",
      "Class balance: 42.7% positive class\n",
      "\n",
      "Step 2: Engineering features...\n",
      "Features selected: 38 common features\n",
      "\n",
      "Step 3: Encoding categorical variables...\n",
      "Categorical columns to encode: 15\n",
      "Final shapes - Training: (177086, 38), Test: (44272, 38)\n",
      "\n",
      "Step 4: Training models...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'train_test_split' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 130\u001b[39m\n\u001b[32m    128\u001b[39m \u001b[38;5;66;03m# Model training\u001b[39;00m\n\u001b[32m    129\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mStep 4: Training models...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m130\u001b[39m X_train, X_val, y_train, y_val = \u001b[43mtrain_test_split\u001b[49m(\n\u001b[32m    131\u001b[39m     X, y, test_size=\u001b[32m0.2\u001b[39m, random_state=\u001b[32m42\u001b[39m, stratify=y\n\u001b[32m    132\u001b[39m )\n\u001b[32m    134\u001b[39m models = {\n\u001b[32m    135\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mRandomForest\u001b[39m\u001b[33m'\u001b[39m: RandomForestClassifier(\n\u001b[32m    136\u001b[39m         n_estimators=\u001b[32m150\u001b[39m, max_depth=\u001b[32m12\u001b[39m, min_samples_split=\u001b[32m15\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    154\u001b[39m     )\n\u001b[32m    155\u001b[39m }\n\u001b[32m    157\u001b[39m validation_scores = {}\n",
      "\u001b[31mNameError\u001b[39m: name 'train_test_split' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Load data\n",
    "print(\"\\nStep 1: Loading datasets...\")\n",
    "train_df = pd.read_excel('kaggle/Training_Dataset.xlsx')\n",
    "test_df = pd.read_excel('kaggle/Testing_Dateset.xlsx')\n",
    "sample_submission = pd.read_excel('kaggle/Sample_Submission.xlsx')\n",
    "\n",
    "print(f\"Training data shape: {train_df.shape}\")\n",
    "print(f\"Test data shape: {test_df.shape}\")\n",
    "print(f\"Sample submission shape: {sample_submission.shape}\")\n",
    "\n",
    "# Correct test IDs to continue from training data\n",
    "test_df['ID'] = range(177087, 177087 + len(test_df))\n",
    "print(f\"Test IDs corrected: {test_df['ID'].min()} to {test_df['ID'].max()}\")\n",
    "\n",
    "# Extract and validate target variable\n",
    "y = train_df['Result'].values.astype(int)\n",
    "print(f\"\\nTarget distribution: Class 0={sum(y==0)}, Class 1={sum(y==1)}\")\n",
    "print(f\"Class balance: {sum(y==1)/len(y)*100:.1f}% positive class\")\n",
    "\n",
    "# Feature engineering function\n",
    "def engineer_features(df):\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Date features\n",
    "    for col in ['Start Date', 'End Date']:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_datetime(df[col], errors='coerce')\n",
    "    \n",
    "    if 'Start Date' in df.columns and 'End Date' in df.columns:\n",
    "        df['Journey_Duration'] = (df['End Date'] - df['Start Date']).dt.days\n",
    "        df['Journey_Duration'] = df['Journey_Duration'].fillna(0).clip(lower=0, upper=365)\n",
    "        \n",
    "        df['Start_Year'] = df['Start Date'].dt.year.fillna(2021)\n",
    "        df['Start_Month'] = df['Start Date'].dt.month.fillna(6)\n",
    "        df['Start_Quarter'] = df['Start Date'].dt.quarter.fillna(2)\n",
    "        df['Start_DayOfWeek'] = df['Start Date'].dt.dayofweek.fillna(3)\n",
    "        \n",
    "        df['End_Year'] = df['End Date'].dt.year.fillna(2021)\n",
    "        df['End_Month'] = df['End Date'].dt.month.fillna(6)\n",
    "        df['End_Quarter'] = df['End Date'].dt.quarter.fillna(2)\n",
    "        df['End_DayOfWeek'] = df['End Date'].dt.dayofweek.fillna(3)\n",
    "        \n",
    "        df = df.drop(['Start Date', 'End Date'], axis=1)\n",
    "    \n",
    "    # Numerical features\n",
    "    if 'Price' in df.columns:\n",
    "        price_median = df['Price'].median()\n",
    "        df['Price'] = df['Price'].fillna(price_median)\n",
    "        df['Price_Log'] = np.log1p(df['Price'].clip(lower=0))\n",
    "        df['Price_Sqrt'] = np.sqrt(df['Price'].clip(lower=0))\n",
    "    \n",
    "    if 'Estimated Win Rate' in df.columns:\n",
    "        df['Estimated Win Rate'] = df['Estimated Win Rate'].fillna(0.5)\n",
    "        df['Win_Rate_Squared'] = df['Estimated Win Rate'] ** 2\n",
    "        df['Win_Rate_High'] = (df['Estimated Win Rate'] > 0.75).astype(int)\n",
    "        df['Win_Rate_Low'] = (df['Estimated Win Rate'] < 0.25).astype(int)\n",
    "    \n",
    "    if 'Unit Number' in df.columns:\n",
    "        df['Unit Number'] = df['Unit Number'].fillna(0)\n",
    "        df['Unit_Log'] = np.log1p(df['Unit Number'])\n",
    "        df['Has_Units'] = (df['Unit Number'] > 0).astype(int)\n",
    "        df['Multiple_Units'] = (df['Unit Number'] > 1).astype(int)\n",
    "    \n",
    "    # Customer segments\n",
    "    segment_cols = [f'Customer Segment {i}' for i in range(1, 6)]\n",
    "    existing_segments = [col for col in segment_cols if col in df.columns]\n",
    "    if existing_segments:\n",
    "        df['Active_Segments'] = df[existing_segments].notna().sum(axis=1)\n",
    "    \n",
    "    # Manager features\n",
    "    if 'Manager' in df.columns:\n",
    "        df['Has_Manager'] = df['Manager'].notna().astype(int)\n",
    "    if 'Techincal Manager' in df.columns:\n",
    "        df['Has_Tech_Manager'] = df['Techincal Manager'].notna().astype(int)\n",
    "    \n",
    "    return df\n",
    "\n",
    "print(\"\\nStep 2: Engineering features...\")\n",
    "train_featured = engineer_features(train_df)\n",
    "test_featured = engineer_features(test_df)\n",
    "\n",
    "# Prepare feature matrices\n",
    "exclude_cols = ['ID', 'Result', 'Customer ID']\n",
    "feature_cols = [col for col in train_featured.columns if col not in exclude_cols]\n",
    "\n",
    "X = train_featured[feature_cols].copy()\n",
    "X_test = test_featured[feature_cols].copy()\n",
    "\n",
    "# Align columns\n",
    "common_cols = sorted(list(set(X.columns) & set(X_test.columns)))\n",
    "X = X[common_cols]\n",
    "X_test = X_test[common_cols]\n",
    "\n",
    "print(f\"Features selected: {len(common_cols)} common features\")\n",
    "\n",
    "# Encode categorical features\n",
    "print(\"\\nStep 3: Encoding categorical variables...\")\n",
    "categorical_cols = X.select_dtypes(include=['object']).columns\n",
    "print(f\"Categorical columns to encode: {len(categorical_cols)}\")\n",
    "\n",
    "for col in categorical_cols:\n",
    "    le = LabelEncoder()\n",
    "    \n",
    "    X[col] = X[col].fillna('missing').astype(str)\n",
    "    X_test[col] = X_test[col].fillna('missing').astype(str)\n",
    "    \n",
    "    all_values = pd.concat([X[col], X_test[col]]).unique()\n",
    "    le.fit(all_values)\n",
    "    \n",
    "    X[col] = le.transform(X[col])\n",
    "    X_test[col] = le.transform(X_test[col])\n",
    "\n",
    "# Handle remaining missing values\n",
    "for col in X.columns:\n",
    "    if X[col].isnull().any():\n",
    "        median_val = X[col].median()\n",
    "        X[col] = X[col].fillna(median_val)\n",
    "        X_test[col] = X_test[col].fillna(median_val)\n",
    "\n",
    "# Convert to arrays\n",
    "X = X.values.astype(np.float32)\n",
    "X_test = X_test.values.astype(np.float32)\n",
    "\n",
    "print(f\"Final shapes - Training: {X.shape}, Test: {X_test.shape}\")\n",
    "\n",
    "# Model training\n",
    "print(\"\\nStep 4: Training models...\")\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "models = {\n",
    "    'RandomForest': RandomForestClassifier(\n",
    "        n_estimators=150, max_depth=12, min_samples_split=15,\n",
    "        min_samples_leaf=8, max_features='sqrt',\n",
    "        random_state=42, n_jobs=-1\n",
    "    ),\n",
    "    'XGBoost': xgb.XGBClassifier(\n",
    "        n_estimators=150, max_depth=6, learning_rate=0.08,\n",
    "        subsample=0.8, colsample_bytree=0.8,\n",
    "        random_state=42, use_label_encoder=False, eval_metric='logloss'\n",
    "    ),\n",
    "    'LightGBM': lgb.LGBMClassifier(\n",
    "        n_estimators=150, max_depth=6, learning_rate=0.08,\n",
    "        num_leaves=40, subsample=0.8, colsample_bytree=0.8,\n",
    "        random_state=42, verbose=-1\n",
    "    ),\n",
    "    'GradientBoosting': GradientBoostingClassifier(\n",
    "        n_estimators=150, max_depth=5, learning_rate=0.08,\n",
    "        subsample=0.8, max_features='sqrt',\n",
    "        random_state=42\n",
    "    )\n",
    "}\n",
    "\n",
    "validation_scores = {}\n",
    "test_probabilities = {}\n",
    "test_binary = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nTraining {name}...\")\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Validation performance\n",
    "    val_prob = model.predict_proba(X_val)[:, 1]\n",
    "    val_binary = model.predict(X_val)\n",
    "    \n",
    "    auc = roc_auc_score(y_val, val_prob)\n",
    "    acc = accuracy_score(y_val, val_binary)\n",
    "    validation_scores[name] = auc\n",
    "    \n",
    "    # Test predictions - both probability and binary\n",
    "    test_prob = model.predict_proba(X_test)[:, 1]\n",
    "    test_bin = model.predict(X_test)\n",
    "    \n",
    "    test_probabilities[name] = test_prob\n",
    "    test_binary[name] = test_bin\n",
    "    \n",
    "    print(f\"  Validation AUC: {auc:.4f}\")\n",
    "    print(f\"  Validation Accuracy: {acc:.4f}\")\n",
    "\n",
    "# Create ensembles\n",
    "print(\"\\nStep 5: Creating ensemble predictions...\")\n",
    "\n",
    "# Calculate weights based on AUC scores\n",
    "total_score = sum(validation_scores.values())\n",
    "weights = {name: score/total_score for name, score in validation_scores.items()}\n",
    "\n",
    "print(\"Model weights based on AUC:\")\n",
    "for name, weight in sorted(weights.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"  {name}: {weight:.3f}\")\n",
    "\n",
    "# Weighted ensemble for probabilities\n",
    "ensemble_prob = np.zeros(len(X_test))\n",
    "for name, weight in weights.items():\n",
    "    ensemble_prob += test_probabilities[name] * weight\n",
    "\n",
    "# Majority voting for binary predictions\n",
    "ensemble_binary_votes = np.array(list(test_binary.values()))\n",
    "ensemble_binary = (ensemble_binary_votes.sum(axis=0) >= len(models)/2).astype(int)\n",
    "\n",
    "# Retrain on full data for final predictions\n",
    "print(\"\\nStep 6: Retraining on full dataset...\")\n",
    "final_probabilities = []\n",
    "final_binary_predictions = []\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"Retraining {name}...\")\n",
    "    model.fit(X, y)\n",
    "    \n",
    "    prob = model.predict_proba(X_test)[:, 1]\n",
    "    binary = model.predict(X_test)\n",
    "    \n",
    "    final_probabilities.append(prob)\n",
    "    final_binary_predictions.append(binary)\n",
    "\n",
    "# Final ensembles\n",
    "final_ensemble_prob = np.mean(final_probabilities, axis=0)\n",
    "final_ensemble_binary = (np.array(final_binary_predictions).sum(axis=0) >= len(models)/2).astype(int)\n",
    "\n",
    "# Create submission files\n",
    "print(\"\\nStep 7: Creating submission files...\")\n",
    "\n",
    "# Submission 1: Probability predictions (for ROC AUC evaluation)\n",
    "submission_probability = pd.DataFrame({\n",
    "    'ID': test_featured['ID'].values,\n",
    "    'TARGET': final_ensemble_prob\n",
    "})\n",
    "submission_probability['TARGET'] = submission_probability['TARGET'].clip(0.0001, 0.9999)\n",
    "\n",
    "# Submission 2: Binary predictions\n",
    "submission_binary = pd.DataFrame({\n",
    "    'ID': test_featured['ID'].values,\n",
    "    'TARGET': final_ensemble_binary\n",
    "})\n",
    "\n",
    "# Save both files\n",
    "submission_probability.to_csv('submission_probability.csv', index=False)\n",
    "submission_binary.to_csv('submission_binary.csv', index=False)\n",
    "\n",
    "# Display statistics\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"SUBMISSION FILE 1: PROBABILITY PREDICTIONS\")\n",
    "print(\"File name: submission_probability.csv\")\n",
    "print(f\"Shape: {submission_probability.shape}\")\n",
    "print(f\"TARGET range: [{submission_probability['TARGET'].min():.4f}, {submission_probability['TARGET'].max():.4f}]\")\n",
    "print(f\"TARGET mean: {submission_probability['TARGET'].mean():.4f}\")\n",
    "print(f\"TARGET std: {submission_probability['TARGET'].std():.4f}\")\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "print(submission_probability.head())\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"SUBMISSION FILE 2: BINARY PREDICTIONS\")\n",
    "print(\"File name: submission_binary.csv\")\n",
    "print(f\"Shape: {submission_binary.shape}\")\n",
    "print(f\"Class distribution: 0={sum(submission_binary['TARGET']==0)}, 1={sum(submission_binary['TARGET']==1)}\")\n",
    "print(f\"Positive class percentage: {sum(submission_binary['TARGET']==1)/len(submission_binary)*100:.1f}%\")\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "print(submission_binary.head())\n",
    "\n",
    "# Comparison visualization\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.hist(submission_probability['TARGET'], bins=50, edgecolor='black', alpha=0.7)\n",
    "plt.title('Probability Distribution')\n",
    "plt.xlabel('Predicted Probability')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.bar(['Class 0', 'Class 1'], \n",
    "        [sum(submission_binary['TARGET']==0), sum(submission_binary['TARGET']==1)],\n",
    "        color=['coral', 'lightgreen'])\n",
    "plt.title('Binary Prediction Distribution')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "threshold = 0.5\n",
    "binary_from_prob = (submission_probability['TARGET'] > threshold).astype(int)\n",
    "confusion = pd.crosstab(submission_binary['TARGET'], binary_from_prob, \n",
    "                        rownames=['Binary Ensemble'], colnames=['Prob > 0.5'])\n",
    "sns.heatmap(confusion, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Binary vs Probability (>0.5) Agreement')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"PIPELINE COMPLETED SUCCESSFULLY\")\n",
    "print(\"\\nTwo submission files created:\")\n",
    "print(\"1. submission_probability.csv - Use this for ROC AUC evaluation\")\n",
    "print(\"2. submission_binary.csv - Binary classification output\")\n",
    "print(\"\\nThe competition requires probability predictions for ROC AUC scoring.\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0911e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28967636",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498bf07e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
